# ğŸš€ Simple Kafka Producer/Consumer Python Project

[![Python](https://img.shields.io/badge/Python-3.8+-blue?logo=python)](https://www.python.org/)
[![Kafka](https://img.shields.io/badge/Apache%20Kafka-KRaft%20Mode-orange?logo=apachekafka)](https://kafka.apache.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-blue?logo=docker)](https://www.docker.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

A minimal **publishâ€“subscribe system** built with **Apache Kafka** (in **KRaft mode**) using **Python** for producing and consuming messages.  
The entire setup runs seamlessly in **Docker Compose**, making it easy to deploy and test locally.

---

## ğŸ“‚ Project Structure

| File | Description |
|------|--------------|
| ğŸ **producer.py** | Sends random product orders to a Kafka topic. |
| ğŸ§  **Listner.py** | Listens to the Kafka topic and prints received orders. |
| ğŸ³ **docker-compose.yml** | Defines the Kafka broker/controller in a single-node setup. |
| ğŸ“¦ **requirements.txt** | Lists required dependencies (`confluent-kafka`). |

---

## ğŸ› ï¸ Prerequisites

Make sure you have the following installed:

- ğŸ‹ **Docker** and **Docker Compose**
- ğŸ **Python 3.8+**

---

## âš™ï¸ 1. Environment Setup

### ğŸ”§ Create and Activate a Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate     # macOS/Linux
venv\Scripts\activate        # Windows

ğŸ“¦ Install Dependencies
pip install -r requirements.txt

ğŸ³ 2. Start the Kafka Cluster

Use Docker Compose to launch the Kafka container:

docker compose up -d


Starts Kafka in KRaft mode (broker + controller in one node).

The service is available at localhost:9092.

You can verify the container is running:

docker ps

ğŸ’» 3. Run the Applications

Youâ€™ll need two terminal windows â€” both with the virtual environment activated.

ğŸ§© A. Start the Consumer (Terminal 1)
python Listner.py


Subscribes to the topic (e.g., orders).

Waits for new messages from Kafka


ğŸš€ B. Start the Producer (Terminal 2)
python producer.py


Youâ€™ll see:

Press Enter to create a new order...


Each time you press Enter, a random order (e.g., "Smartphone") is generated and sent to Kafka.

ğŸ” C. Observe the Data Flow

When the producer sends a message, the consumer instantly receives it:

ğŸ“¦ Received order: Smartphone


âœ… Congratulations! You just built a working Kafka Producerâ€“Consumer pipeline in Python.

ğŸ›‘ 4. Cleanup

When finished:

ğŸ§¹ Stop the Python Scripts

Press Ctrl + C in both terminals.

ğŸ‹ Stop the Docker Containers
docker compose down


This will stop the Kafka container and remove associated resources.


ğŸ’¡ Tips & Next Steps

You can extend this project by:

Adding multiple consumers to test Kafkaâ€™s load balancing.

Creating custom topics for various data streams.

Implementing JSON serialization/deserialization for structured data.

Using Avro or Protobuf with a Schema Registry.

ğŸ§  Summary

This project provides a simple yet powerful introduction to how Kafka works in a publish/subscribe model with Python clients.
Using Docker Compose makes local experimentation fast and consistent.

ğŸ‘¨â€ğŸ’» Author

Mohammed Yassine Bakhtaoui