<div align="center">

# ğŸš€ **Simple Kafka Producer/Consumer Python Project**

[![Python](https://img.shields.io/badge/Python-3.8+-blue?logo=python)](https://www.python.org/)
[![Kafka](https://img.shields.io/badge/Apache%20Kafka-KRaft%20Mode-orange?logo=apachekafka)](https://kafka.apache.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-blue?logo=docker)](https://www.docker.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

</div>

<p align="center">
A **minimal** and **hands-on** publishâ€“subscribe system built with âš™ï¸ <b>Apache Kafka</b> (KRaft mode) using <b>Python</b> clients â€” all running smoothly inside <b>Docker Compose</b> ğŸ‹.
</p>

---

## ğŸ—ï¸ Project Overview

This project demonstrates the **producerâ€“consumer** model with Kafka:  
One script produces random product orders, and another consumes them in real-time.

---

## ğŸ“‚ Folder Structure

| ğŸ—ƒï¸ File | ğŸ§© Description |
|------|--------------|
| ğŸ **producer.py** | Generates and sends random product orders to Kafka. |
| ğŸ§  **Listner.py** | Consumes messages from the Kafka topic and prints them in real-time. |
| ğŸ³ **docker-compose.yml** | Single-node Kafka setup (broker + controller with KRaft mode). |
| ğŸ“¦ **requirements.txt** | Python dependencies (`confluent-kafka`). |

---

## âš™ï¸ Prerequisites

Before running, ensure you have:

- ğŸ‹ **Docker** & **Docker Compose**
- ğŸ **Python 3.8+**

> ğŸ’¡ Tip: Check your installations with `docker --version` and `python --version`.

---

## ğŸ§© Step 1 â€” Environment Setup

### ğŸ”¹ Create & Activate a Virtual Environment
```bash
python3 -m venv venv
# Activate on macOS/Linux
source venv/bin/activate
# Activate on Windows
venv\Scripts\activate
```

### ğŸ”¹ Install Dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸ³ Step 2 â€” Start the Kafka Cluster

Launch Kafka with Docker Compose:

```bash
docker compose up -d
```

âœ… **Kafka is ready!**  
Runs in **KRaft mode** (broker + controller in one node).  
Accessible on **localhost:9092**.

Check the container status:
```bash
docker ps
```

---

## ğŸ’» Step 3 â€” Run the Project

Youâ€™ll need **two terminals** (both with the virtual environment activated).

### ğŸ§  Terminal 1: Start the Consumer
```bash
python Listner.py
```

ğŸŸ¢ Waits for messages from Kafka.

### âš¡ Terminal 2: Start the Producer
```bash
python producer.py
```

Youâ€™ll see:
```
Press Enter to create a new order...
```

Each time you press **Enter**, a random order (e.g. _â€œSmartphoneâ€_) gets published to Kafka.  
Meanwhile, the consumer will instantly print:

```
ğŸ“¦ Received order: Smartphone
```

---

## ğŸ§¹ Step 4 â€” Cleanup

### ğŸ”¸ Stop the Scripts
Press **Ctrl + C** in both terminals.

### ğŸ”¸ Shutdown Kafka
```bash
docker compose down
```

This will stop and remove all Kafka containers and volumes.

---

## ğŸŒ± Next Steps & Ideas

Want to power this up? Here are some ideas:
- âš–ï¸ Add multiple consumers to test Kafka load-balancing.
- ğŸ§¾ Use JSON serialization for rich, structured messages.
- ğŸ§© Experiment with Avro/Protobuf + Schema Registry.
- ğŸ§  Integrate monitoring tools like Kafdrop or Prometheus.

---

## ğŸ§  Summary

ğŸ”¥ Youâ€™ve built a fully working **Kafka Producerâ€“Consumer** pipeline in Python!  
Using **Docker Compose** makes running and testing fast, isolated, and repeatable.  
This setup gives you a solid foundation for learning **event-driven architectures** and real-time data streaming.

---

<div align="center">

ğŸ‘¨â€ğŸ’» **Author:** [Mohammed Yassine Bakhtaoui](#)  
ğŸªª **License:** MIT  
ğŸ’™ *Made with Python, Kafka, and ğŸš€ enthusiasm!*

</div>